Problem Statement

Imagine you are working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without 
using a remote.

The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:
  Thumbs up:  Increase the volume
  Thumbs down: Decrease the volume
  Left swipe: 'Jump' backwards 10 seconds
  Right swipe: 'Jump' forward 10 seconds  
  Stop: Pause the movie
  
Each video is a sequence of 30 frames (or images).
 
Understanding the Dataset
The training data consists of a few hundred videos categorised into one of the five classes. Each video (typically 2-3 seconds long) 
is divided into a sequence of 30 frames(images). These videos have been recorded by various people performing one of the five gestures 
in front of a webcam - similar to what the smart TV will use.

The data is in a zip file. The zip file contains a 'train' and a 'val' folder with two CSV files for the two folders. These folders 
are in turn divided into subfolders where each subfolder represents a video of a particular gesture. Each subfolder, i.e. a video, 
contains 30 frames (or images). Note that all images in a particular video subfolder have the same dimensions but different videos
may have different dimensions. Specifically, videos have two types of dimensions - either 360x360 or 120x160 (depending on the webcam 
used to record the videos). Hence, you will need to do some pre-processing to standardise the videos. 
 

Each row of the CSV file represents one video and contains three main pieces of information - the name of the subfolder containing 
the 30 images of the video, the name of the gesture and the numeric label (between 0-4) of the video.
 

Your task is to train a model on the 'train' folder which performs well on the 'val' folder as well (as usually done in ML projects). 
We have withheld the test folder for evaluation purposes - your final model's performance will be tested on the 'test' set.
 

To get started with the model building process, you first need to get the data on your persistent storage. The next lecture will 
help you in getting the data from the google drive link to the Paperspace persistent storage.

In order to get the data on the persistent storage, perform the following steps in order.
  Download the above zip file containing the Perl script and upload it on your persistent storage in a folder of your choice
  Open the terminal
  Go to the path where you have kept the zip file on the terminal and execute 'unzip gdown.pl-master.zip'. You'll find an unzipped 
  folder 'gdown.pl-master'.
  Run the following commands in order:
      apt-get remove wget
      apt-get update
      apt-get upgrade
      apt-get dist-upgrade
      apt-get install wget
  Now, move to the folder 'gdown.pl-master' on the terminal and 
  execute './gdown.pl https://drive.google.com/file/d/1ehyrYBQ5rbQQe6yL4XbLWe3FMvuVUGiL/view?usp=sharing Neural_Nets.zip'
  Execute 'unzip Neural_Nets.zip' and there you have the data with you.
